{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2019_04_24_VI_PYMC3의 사본",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jikhans/python_algorthm_basic/blob/master/2019_04_24_VI_PYMC3%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "061nODp5y3J5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "VI : https://medium.com/@albertoarrigoni/scalable-bayesian-inference-in-python-a6690c7061a3"
      ]
    },
    {
      "metadata": {
        "id": "sv92LaYey5cy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Variational Inference in PyMC3"
      ]
    },
    {
      "metadata": {
        "id": "cvsvWcFxzBgg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pymc3 as pm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def logistic(x, b, noise =None):\n",
        "  L = x.T.dot(b)\n",
        "  if noise is not None:\n",
        "    L = L + noise # L may likelihood\n",
        "  return 1/(1+np.exp(-L))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VKVZzUO620mw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x1 = np.linspace(-10., 10, 10000)\n",
        "x2 = np.linspace(0., 20, 10000)\n",
        "bias = np.ones(len(x1))\n",
        "X = np.vstack([x1,x2,bias])\n",
        "B = [-10., 2., 1.] # sigmoid params for X + intercept\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fQThR7cL7WoU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* noise mean"
      ]
    },
    {
      "metadata": {
        "id": "vTg_hTgP7YFi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pnoisy = logistic(X,B, noise = np.random.normal(loc=0., scale=0., size = len(x1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ERc18VY7gpf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " dichotomize pnoisy -- sample 0/1 with probability pnoisy"
      ]
    },
    {
      "metadata": {
        "id": "mNXO8jt17e7g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y =np.random.binomial(1., pnoisy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SgMMit-I70K0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*  What we are doing here is just creating two variables (x1, x2) whose linear combination is run through a sigmoid function.\n",
        "\n",
        "* after that we sample from a Binomial distribution with parameter p defined by the sigmoid output. \n",
        "\n",
        "* The coefficients (betas) of the model are stored in the list ‘B’. \n",
        "\n",
        "* At this point we use Pymc3 to define a probabilistic model for logistic regression and try to obtain a posterior distribution for each of the parameters (betas) defined above."
      ]
    },
    {
      "metadata": {
        "id": "OzBL2z3e7z7V",
        "colab_type": "code",
        "outputId": "8ecfcad1-aff5-46c9-ad7d-cadfdc7353c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "cell_type": "code",
      "source": [
        "with pm.Model() as model:\n",
        "  # define prior\n",
        "  intercept = pm.Normal('Intercept',0, sd=10)\n",
        "  x1_coef = pm.Normal('x1', 0, sd=10)\n",
        "  x2_coef = pm.Normal('x2', 0, sd=10)\n",
        "  \n",
        "  # Define likelihood\n",
        "  \n",
        "  likelhood = pm.Bernoulli('y',\n",
        "                         pm.math.sigmoid(intercept + x1_coef*X[0] + x2_coef*X[1]), observed = y)\n",
        "  trance = pm.sample(3000)\n",
        "  #pm.traceplot(trace)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Auto-assigning NUTS sampler...\n",
            "Initializing NUTS using jitter+adapt_diag...\n",
            "Sequential sampling (2 chains in 1 job)\n",
            "NUTS: [x2, x1, Intercept]\n",
            "100%|██████████| 3500/3500 [08:45<00:00,  5.34it/s]\n",
            "100%|██████████| 3500/3500 [09:11<00:00,  6.66it/s]\n",
            "The number of effective samples is smaller than 25% for some parameters.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "gwqKq1Zr9xSx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* The results are approximately what we expected: the maximum a posteriori (MAP) estimation coincides with the ‘beta’ parameters we used for data generation. "
      ]
    },
    {
      "metadata": {
        "id": "-b5qGKu191pH",
        "colab_type": "code",
        "outputId": "2f0ff851-53ac-487a-c4d9-348bc9d2fe9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "cell_type": "code",
      "source": [
        "# AVDI\n",
        "\n",
        "from pymc3.variational.callbacks import CheckParametersConvergence\n",
        "\n",
        "with model:\n",
        "  fit = pm.fit(100_000, method ='advi', callbacks =[CheckParametersConvergence()])\n",
        "  \n",
        "draws = fit.sample(2_000) # This will automatically check parameters converge\n",
        "\n",
        "import arviz as az\n",
        "\n",
        "az.plot_forest([draws, trace])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Loss = 232.84:  54%|█████▍    | 54372/100000 [01:56<01:29, 509.05it/s]\n",
            "Convergence achieved at 54400\n",
            "Interrupted at 54,399 [54%]: Average Loss = 1,982.9\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-0e1d2234aa41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdraws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2_000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This will automatically check parameters converge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0marviz\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0maz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0maz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_forest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdraws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'arviz'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "TtMBBOx7COcD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "az.plot_pair(trace, figsize =(5,5)) # covariance plots for the NUTS trace"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kZv-xqZ7CWEZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "az.plot_pair(draws, figsize =(5,5)) # covariance plots for the ADVI trace"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FpF2I_5X-rtx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* reference: https://stackoverflow.com/questions/52558826/why-is-pymc3-advi-worse-than-mcmc-in-this-logistic-regression-example"
      ]
    },
    {
      "metadata": {
        "id": "5L5LA2El-7qN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* ADVI is clearly underestimating the variance, but it is fairly close to the mean of each parameter.\n",
        "* Let us try to visualize the covariance structure of the model to understand where this lack of precision may come from"
      ]
    },
    {
      "metadata": {
        "id": "wKjic2jO_tcg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Clearly, ADVI does not capture (as expected) the interactions between variables because of the mean field approximation, \n",
        "\n",
        "* and so it underestimates the overall variance by far (be advised: this is a particularly tricky example chosen to highlight this kind of behavior).\n",
        "\n",
        "* ADVI is a very convenient inferential procedure that let us characterize complex posterior distributions in a very short time (if compared to Gibbs/MCMC sampling). \n",
        "\n",
        "* The solution it finds is a distribution which approximate the posterior, although it may not converge to the real posterior: for most cases this may not be a problem,\n",
        "\n",
        "* but we may need to pay extra-attention in cases where the covariance structure of the variables is crucial "
      ]
    }
  ]
}